id,ru,en
base1,WebVectors: семантические модели для русского языка,WebVectors: distributional semantic models online
base2,Показать/скрыть меню,Toggle Navigation
base3,WebVectors,WebVectors
base4,О&nbsp;проекте,About
base5,Калькулятор,Calculator
base6,Похожие слова,Similar words
base7,Модели,Models
base8,Обучить свою модель,Train your model
base9,Публикации,Publications
base10,Контакты,Contacts
base11,Команда WebVectors,WebVectors Team
base12,Лицензия Creative Commons,Creative Commons License
base13,Визуализации,Visualizations
base14,Университет Осло, University of Oslo
base15,Национальный Исследовательский Университет Высшая Школа Экономики,National Research University Higher School of Economics
base16,Switch language,Сменить язык
base17,Различные операции,Miscellaneous
base18,"Слова, выделенные <span style='color: green;'>зеленым</span>, являются высокочастотными (доля в корпусе выше 0.00001); слова, выделенные <span style='color: red;'>красным</span>, являются низкочастотными (доля в корпусе ниже 0.0000005).","Words in <span style='color: green;'>green</span> are top frequent (corpus ratio higher than 0.00001); words in <span style='color: red;'>red</span>, are low frequent (corpus ratio less than 0.0000005)."
base19,"Показаны только ассоциаты той же части речи, что и слово в запросе. Изменить этот фильтр можно на вкладке <a href=""/similar"">Похожие слова</a>.","We show only the associates of the same part of speech as your query. All associates can be found at the <a href=""/similar"">Similar Words</a> tab."
calc1,Семантический калькулятор,Semantic Calculator
calc2,Алгебраические операции,Algebraic operations
calc3,"Введите в&nbsp;&laquo;<strong>положительную</strong>&raquo; и&nbsp;&laquo;<strong>отрицательную</strong>&raquo; формы не&nbsp;более 10&nbsp;слов через пробел. <i>WebVectors</i> сложит вектора положительных слов и&nbsp;вычтет из&nbsp;них отрицательные. Затем он&nbsp;выдаст слова, наиболее близкие к&nbsp;получившемуся вектору. Если вы&nbsp;оставите отрицательное поле пустым, <i>WebVectors</i> просто найдет центр лексического кластера, образованного положительными словами.","Enter not more than 10&nbsp;space-separated words into <strong>positive</strong> and <strong>negative</strong> forms. <i>WebVectors</i> will sum up&nbsp;vectors for the positive words and subtract vectors from the negative ones. Then it&nbsp;will output the word closest to&nbsp;the resulting vector. If&nbsp;you leave negative form empty, <i>WebVectors</i> will simply find the center of&nbsp;word cluster formed by&nbsp;your positive words."
calc4,"Здесь&nbsp;можно вычислять отношения: например, &laquo;<strong>найти слово&nbsp;D, связанное со&nbsp;словом&nbsp;C таким&nbsp;же образом, как слово&nbsp;A связано со&nbsp;словом B</strong>&raquo;. Таким образом можно определять семантические связи между понятиями и решать задачи на аналогии. В&nbsp;форме ввода приведен пример: какое слово относится к&nbsp;слову <strong>&laquo;Лондон&raquo;</strong>, так&nbsp;же, как <strong>&laquo;Россия&raquo;</strong> относится к&nbsp;<strong>&laquo;Москве&raquo;</strong>? Ответ&nbsp;&#8212; &laquo;<strong>Великобритания</strong>&raquo;: Лондон столица Великобритании, а&nbsp;Москва&nbsp;&#8212; столица России. <a onclick =""javascript:ShowHide('HiddenDiv')"" href=""javascript:;"" >Подробнее...</a></p>
<div class=""text"" id=""HiddenDiv"" style=""DISPLAY: none"" ><p><small>Можно сказать, что это вычитание компоненты &laquo;москва&raquo; из&nbsp;семантики слова &laquo;россия&raquo; и&nbsp;добавление компоненты &laquo;лондон&raquo;. Модель приходит к&nbsp;выводу, что &laquo;Россия для Лондона&nbsp;&#8212; это Великобритания&raquo; и&nbsp;выдает &laquo;Великобританию&raquo; в&nbsp;качестве ответа.</small></p>
<p><small>К&nbsp;словам можно дописывать символ подчеркивания &laquo;_&raquo; и&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"" data-toggle=""tooltip"" data-placement=""top"" title=""NOUN, PROPN, VERB, ADJ..."">тэг части речи</a> (<i>&laquo;река_NOUN&raquo;</i>). В&nbsp;ином случае, <i>RusVectōrēs</i> определит часть речи автоматически.</small></p></div>","Calculate ratios, such as&nbsp;&laquo;<strong>find a&nbsp;word&nbsp;D related to&nbsp;the word&nbsp;C in&nbsp;the same way as&nbsp;the word&nbsp;A is&nbsp;related to&nbsp;the word B</strong>&raquo;. An&nbsp;example is&nbsp;given in&nbsp;the placeholder: which word is&nbsp;in&nbsp;the same relation to&nbsp;the word &laquo;<strong>Лондон</strong>&raquo; as&nbsp;&laquo;<strong>Россия</strong>&raquo; is&nbsp;to&nbsp;&laquo;<strong>Москва</strong>&raquo;? The answer is&nbsp;&laquo;<strong>Великобритания</strong>&raquo;: these concepts are in&nbsp;identical capital relations. <a onclick =""javascript:ShowHide('HiddenDiv')"" href=""javascript:;"" >More on this...</a></p>
<div class=""text"" id=""HiddenDiv"" style=""DISPLAY: none"" ><p><small>You can also think of&nbsp;this operation as&nbsp;removing the &laquo;Москва&raquo; component from the semantics of&nbsp;&laquo;Россия&raquo; and augmenting it&nbsp;with &laquo;Лондон&raquo; component. The model inferences that it&nbsp;should change the&nbsp;country. Thus, it&nbsp;outputs &laquo;Великобритания&raquo; as&nbsp;an&nbsp;answer.</small></p></div>"
calc5,"считает, что это будет:",thinks it&nbsp;will&nbsp;be:
calc7,НКРЯ,Ruscorpora
calc8,Русская Wikipedia,Russian Wikipedia
calc9,НКРЯ и&nbsp;русская Wikipedia,Ruscorpora and Russian Wikipedia
calc10,Веб-корпус,Web corpus
calc11,Новостной корпус,News corpus
calc13,computer linguistics,computer linguistics
calc18,Вычислить!,Calculate!
calc21,"Вы&nbsp;можете приписать к&nbsp;слову знак подчеркивания &laquo;_&raquo; и&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"" data-toggle=""tooltip"" data-placement=""top"" title=""NOUN, PROPN, VERB, ADJ..."">тэг части речи</a> (<i>&laquo;tea_NOUN&raquo;</i>). В&nbsp;ином случае <i>WebVectors</i> определит часть речи самостоятельно.","You can optionally end the words with an&nbsp;underscore and a&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"" data-toggle=""tooltip"" data-placement=""top"" title=""NOUN, PROPN, VERB, ADJ..."">PoS tag</a> (<i>&laquo;tea_NOUN&raquo;</i>). Otherwise, <i>WebVectors</i> will analyze words on&nbsp;its own."
calc23,НКРЯ,Ruscorpora
calc24,Русская Wikipedia,Russian Wikipedia
calc25,НКРЯ и&nbsp;русская Википедия,Ruscorpora and Russian Wikipedia
calc26,Веб-корпус,Web corpus
calc27,Новостной корпус,News corpus
calc31,Относится&nbsp;к,Relates&nbsp;to
calc32,"Вы&nbsp;также можете попробовать более сложные операции над векторами, чем простое решение пропорции.","If&nbsp;you feel confident with algebraic operations on&nbsp;vectors, you can try something more sophisticated than simple analogical inference."
calc33,mother,mother
calc34,daughter,daughter
calc35,father,father
description1,"РусВекторес: дистрибутивная семантика для русского языка, веб-интерфейс и модели для скачивания","WebVectors: word embeddings, web interface and models to download"
home1,WebVectors: семантические модели для русского языка,WebVectors: word embeddings online
home2,"Введите слово, чтобы получить список из&nbsp;10&nbsp;его ближайших семантических аналогов (квази-синонимов). <br/>Будет использована модель, обученная на русской Википедии и Национальном корпусе русского языка; другие модели вы можете найти на вкладке <a href=""similar/"">Похожие слова</a>.","Enter a&nbsp;word to&nbsp;produce a&nbsp;list of&nbsp;its 10&nbsp;nearest semantic associates.<br/>English Wikipedia model will be&nbsp;used; for other models, visit <a href=""similar"">Similar Words</a> tab."
home3,Найти похожие слова!,Find similar words!
home4,Семантические аналоги для,Semantic associates for
home5,вычисленные на&nbsp;модели,computed&nbsp;on
home6,&#8217;You shall know a&nbsp;word by&nbsp;the company it&nbsp;keeps.&#8217; (Firth 1957).,&#8217;You shall know a&nbsp;word by&nbsp;the company it&nbsp;keeps.&#8217; (Firth 1957)
home7,computer_NOUN,computer_NOUN
similar0,Araneum fastText,Araneum fastText
similar1,Вычисление семантических ассоциатов,Computing associates
similar2,"Введите слово, чтобы получить список из&nbsp;10&nbsp;его ближайших семантических аналогов (квази-синонимов). Вы&nbsp;можете приписать к&nbsp;слову знак подчеркивания &laquo;_&raquo; и&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"" data-toggle=""tooltip"" data-placement=""top"" title=""NOUN, PROPN, VERB, ADJ..."" target=""_blank"">тэг части речи</a> (<i>&laquo;tea_NOUN&raquo;</i>). Если вы&nbsp;этого не&nbsp;сделаете, <i>WebVectors</i> определит часть речи автоматически.","Enter a&nbsp;word to&nbsp;produce a&nbsp;list of&nbsp;its 10&nbsp;nearest semantic associates (quazy-synonyms). You can optionally end the word with an&nbsp;underscore and a&nbsp;<a href=""http://universaldependencies.org/u/pos/all.html"" data-toggle=""tooltip"" data-placement=""top"" title=""NOUN, PROPN, VERB, ADJ..."" target=""_blank"">PoS tag</a> (<i>&laquo;tea_NOUN&raquo;</i>). Otherwise, <i>WebVectors</i> will detect&nbsp;it."
similar3,Выберите модель:,Choose the model:
similar4,НКРЯ,Ruscorpora
similar5,Английская Wikipedia,English Wikipedia
similar6,Британский Национальный Корпус,British National Corpus
similar7,Норвежский новостной корпус,Norsk Aviskorpus
similar8,Английский Gigaword,English Gigaword
similar9,Показывать только:,Show only:
similar10,Существительные,Nouns
similar11,Глаголы,Verbs
similar12,Наречия,Adverbs
similar13,Прилагательные,Adjectives
similar14,Все части речи,All of&nbsp;them
similar15,Найти похожие слова!,Find similar words!
similar16,Cемантические аналоги для,Semantic associates for
similar17,вычислено на&nbsp;модели,computed on&nbsp;data from
similar18,Модели неизвестно слово,The model does not know the word
similar19,Часть речи запроса,Query part of&nbsp;speech
similar20,Имена собственные,Proper names
similar21,Некорректный запрос,Incorrect query
similar22,Некорректный тэг,Incorrect tag
similar23,Нет подходящих результатов,No&nbsp;results
similar24,Вычисление семантической близости,Computing similarity
similar25,"Введите через пробел 2&nbsp;слова, чтобы вычислить их&nbsp;семантическое сходство. Можно также ввести несколько пар, разделяя их&nbsp;запятыми, как в&nbsp;примере.","Enter 2&nbsp;space-separated words to&nbsp;calculate their similarity. It&nbsp;is&nbsp;also possible to&nbsp;enter several pairs separating them with commas, as&nbsp;in&nbsp;the placeholder."
similar26,"movie film, city town","movie film, city town"
similar27,Вычислить семантическую близость!,Compute semantic similarity!
similar28,Пары слов,Word pairs
similar29,Косинусная близость,Cosine similarity
similar30,Нет близких слов с&nbsp;такой частью речи,No&nbsp;similar words with this tag
similar31,История запросов близости,Similarity queries history
similar32,Слова нет в словаре модели; вектор восстановлен из составляющих его символов.,The word is out of model vocabulary; its embedding is inferred from its characters.
similar33,Тайга,Taiga
similar34,"Для каждого слова показана его часть речи (если в модели они размечены).","Parts of speech are shown for each word, if present in the model."
synraw1,"Слова, семантически связанные&nbsp;с",Semantically related words for
synraw2,Какие слова близки к&nbsp;слову,What words are related&nbsp;to
synraw3,в,in&nbsp;the
synraw4,Показать вектор,Show the raw vector&nbsp;of
synraw5,в&nbsp;модели,in&nbsp;model
synraw6,Поискать,Search
synraw7,в&nbsp;Интернете,in&nbsp;the Internet
synraw8,на&nbsp;Wiktionary,in&nbsp;the Wiktionary
synraw9,О&nbsp;слове,About the word
synraw10,&#8212;&nbsp;визуализация вектора; по&nbsp;клику доступна полноразмерная версия,vector plot; click for full-size image
synraw11,в&nbsp;Википедии,in&nbsp;Wikipedia
synraw12,Это слово в&nbsp;других моделях,This word in&nbsp;other models
synraw13,в&nbsp;НКРЯ,in&nbsp;the RNC
synraw14,частота в корпусе,corpus frequency
synraw15,часть речи, part of speech
upload10,"Когда обучение закончится, вы&nbsp;сможете скачать вашу модель здесь:","When the training is&nbsp;finished, you can download your model here:"
upload11,Размерность векторов:,Vector size:
upload12,Алгоритм обучения:,Training mode:
upload13,Размер симметричного окна:,Symmetric window size:
upload1,Обучите свою модель,Train your model
upload2,Загрузите свой корпус!,Upload your own corpus!
upload3,"Введите URL, с&nbsp;которого можно загрузить ваш обучающий корпус. Корпус должен представлять собой txt-файл в&nbsp;кодировке UTF-8 (без некорректных символов), упакованный gzip, и&nbsp;содержащий одно предложение на&nbsp;каждой строке.",Enter a&nbsp;direct URL from where your training corpus can be&nbsp;downloaded. It&nbsp;should be&nbsp;a&nbsp;gzipped plain text UTF-8 (with no&nbsp;incorrect characters) document with one sentence per line.
upload4,Обработать,Process
upload5,Ваш файл,Your file from
upload6,отправлен в&nbsp;очередь на&nbsp;загрузку и&nbsp;обучение.,was put into downloading and training queue.
upload7,Ваш идентификатор обучения&nbsp;&#8212;,Your training identifier&nbsp;is
upload8,"Пожалуйста, сохраните его в&nbsp;надежном месте.","Please, write it&nbsp;down somewhere."
upload9,"Обучение вашей модели займет некоторое время. Типичная скорость обработки&nbsp;&#8212; около 50&nbsp;тысяч слов в&nbsp;секунду (иногда чуть медленнее, в&nbsp;зависимости от&nbsp;многих факторов).",It&nbsp;will take a&nbsp;while to&nbsp;train your model. General safe rule of&nbsp;thumb as&nbsp;for processing speed is&nbsp;approximately 50&nbsp;thousand words a&nbsp;second (plus some additional time for various system pipelines).
usermodel1,"Модели, созданные пользователями",User-generated models
usermodel2,Обучение модели на&nbsp;вашем корпусе завершено,Model training on&nbsp;your corpus has finished.
usermodel3,Здесь вы&nbsp;можете скачать вашу модель в&nbsp;бинарном формате Word2Vec (правая кнопка на&nbsp;ссылке&nbsp;&#8212; Скачать как):,Here you can download your model in&nbsp;binary Word2Vec format (right-click on&nbsp;the link below and press Save Link&nbsp;As):
usermodel4,Ваша модель с&nbsp;идентификатором,Your model with identifier
usermodel5,все еще обрабатывается. Приходите попозже.,is&nbsp;still being processed. Please come back later.
usermodel6,"Извините, идентификатор не&nbsp;распознан.","Unknown identifier, sorry."
visual1,Визуализация семантических связей между словами,Visualizing word inter-relations
visual2,Введите cлова через запятую. Мы&nbsp;построим карту их&nbsp;взаимного расположения в&nbsp;выбранной модели/моделях и&nbsp;отобразим двумерную проекцию этой карты (из&nbsp;векторного пространства высокой размерности).,"Enter a&nbsp;comma-separated list of&nbsp;words. We&nbsp;will build a&nbsp;map of&nbsp;their inter-relations in&nbsp;the chosen model(s), and return 2-dimensional version of&nbsp;this map (projected from high-dimensional vector space)."
visual3,Визуализация взаимного расположения слов при помощи t-SNE,Visual representation of&nbsp;word relations using t-SNE
visual4,Следующие слова неизвестны модели:,The following words were unknown to&nbsp;the model:
visual5,&#8212;&nbsp;это алгоритм снижения размерности и&nbsp;визуализации высокоразмерных данных. Он&nbsp;разработан Лоренсом ван дер Маатеном и&nbsp;описан в&nbsp;этой статье:,"is&nbsp;an&nbsp;algorithm for dimensionality reduction and visualization of&nbsp;high-dimensional datasets, developed by&nbsp;Laurens van der Maaten and described in&nbsp;this paper:"
visual6,Визуализация отношений между словами; по&nbsp;клику доступна полноразмерная версия,Plot for word relations; click for full-size image
visual7,"car,tank,transport,computer,mouse,Moscow,Paris,London,Russia,France,Britain,clean,dirty,new","car,tank,transport,computer,mouse,Moscow,Paris,London,Russia,France,Britain,clean,dirty,new"
visual8,Слишком мало слов,Too few words
visual9,Слова не&nbsp;должны повторяться,Words must be&nbsp;unique
visual15,Визуализировать,Visualize
visual16,Визуализация,Visualization
visual17,вычислено на&nbsp;модели,computed on&nbsp;data from
visual18,14&nbsp;тысяч наиболее частотных существительных в&nbsp;НКРЯ,14K most frequent nouns in&nbsp;the Russian National Corpus
visual19,Визуализация отношений между cуществительными; по&nbsp;клику доступна полноразмерная версия,Plot for noun relations; click for full-size image
visual20,Примеры больших семантических карт,Examples of&nbsp;large semantic maps
visual21,8&nbsp;тысяч наиболее частотных существительных в&nbsp;новостном корпусе,8K&nbsp;most frequent nouns in&nbsp;the news corpus
visual22,Визуализировать в&nbsp;TensorFlow Projector,Visualize in&nbsp;TensorFlow Projector
visual23,Показать трёхмерную проекцию взаимного расположения выбранных слов в&nbsp;TensorFlow Projector от&nbsp;Google,Show 3D&nbsp;projection of&nbsp;the chosen words in&nbsp;Google&#8217;s TensorFlow Projector
visual24,"Вы&nbsp;можете добавлять новые группы слов кнопкой &#8217;+&#8217;; они отобразятся на&nbsp;визуализации разными цветами (если группа одна, цвета соответствуют частям речи). Оптимальное общее количество слов&nbsp;&#8212; от&nbsp;7&nbsp;до&nbsp;20.","You can add new groups of&nbsp;words with the &#8217;+&#8217; button. They will be&nbsp;visualized with different colors (if&nbsp;there is&nbsp;only 1&nbsp;group, the colors marks parts of&nbsp;speech). Optimal total number of&nbsp;words is&nbsp;from 7&nbsp;to&nbsp;20."
frequency1,Частотность,Ratio
frequency2,Высокая,High
frequency3,Средняя,Medium
frequency4,Низкая,Low
frequency5,Доля в корпусе выше 0.00001,Corpus ratio higher than 0.00001
frequency6,Доля в корпусе от 0.0000005 до 0.00001,Corpus ratio from 0.0000005 to 0.00001
frequency7,Доля в корпусе ниже 0.0000005,Corpus ratio less than 0.0000005